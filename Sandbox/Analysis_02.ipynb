{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Computing Sharpe Ratio on existing shares allocation </b>\n",
    "\n",
    "<pre>\n",
    "Symbol  Shares  Acquired\n",
    "AAPL    420     2014-07-07\n",
    "GOOG    17      2014-06-26\n",
    "TSLA    51      2015-03-03\n",
    "TWTR    1396    2014-07-02\n",
    "UNH     1464    2014-07-07\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stocks_dir = '/Volumes/Photos/stocks/'\n",
    "stocks_dir = './'\n",
    "\n",
    "symbols = ['UNH', 'AAPL', 'GOOG', 'TSLA', 'TWTR']\n",
    "\n",
    "stocks = {}\n",
    "\n",
    "for s in symbols:\n",
    "    file_name = stocks_dir + s + '.csv'\n",
    "    df = pd.read_csv(file_name, index_col='Date', parse_dates=True, usecols=['Date', 'Adj. Close'])\n",
    "    stocks[s] = df\n",
    "\n",
    "\n",
    "sp500 = pd.read_csv('SP500_yahoo.csv', index_col='Date', parse_dates=True, na_values=['.'])\n",
    "sp500 = sp500[['Adj Close']]\n",
    "sp500.columns = ['Adj. Close']\n",
    "\n",
    "symbols.extend(['SP500'])\n",
    "stocks['SP500']  = sp500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares = {'UNH': 1464, 'AAPL': 420, 'GOOG': 17, 'TSLA': 51, 'TWTR': 1396, 'SP500': 0}\n",
    "\n",
    "for sym, df in stocks.items():\n",
    "    df['shares'] = shares[sym]\n",
    "    #df['daily_ret'] = df['Adj. Close'].pct_change(1)\n",
    "    df['log_ret'] = np.log(df['Adj. Close']/df['Adj. Close'].shift(1))\n",
    "    df['alloc'] = df['shares'] * df['Adj. Close']\n",
    "    df.drop(columns=['shares', 'Adj. Close'], inplace=True)\n",
    "    df.columns = [ sym + ' ' + c for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = '2014-04-01', '2018-03-15'\n",
    "df = pd.concat(stocks.values(), axis=1).loc[d1:d2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alloc'] = sum([df[sym + ' alloc'] for sym in symbols])\n",
    "\n",
    "for sym in symbols:\n",
    "    df[sym + ' weight'] = df[sym + ' alloc'] / df['alloc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['2014-07-02':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = '2014-07-02', '2018-01-01'\n",
    "#d1, d2 = '2017-01-01', '2018-01-01'\n",
    "#d1, d2 = '2015-03-01', '2015-06-01'\n",
    "\n",
    "days = abs((parse(d1) - parse(d2)).days)\n",
    "print(\"Real Return: {} x100(%)\".format( (df.loc[d1:d2].iloc[-1]['alloc'] / df.loc[d1:d2].iloc[0]['alloc']) ** ( 365/days ) - 1))\n",
    "\n",
    "df2 = df.loc[d1:d2]\n",
    "\n",
    "# Expected Return\n",
    "log_ret = df2[[sym + ' log_ret' for sym in symbols]]\n",
    "log_ret.columns = range(len(log_ret.columns))\n",
    "#print(log_ret.head())\n",
    "\n",
    "weights = df2[[sym + ' weight' for sym in symbols]]\n",
    "weights.columns = range(len(weights.columns))\n",
    "#print(weights.head())\n",
    "\n",
    "exp_ret = (log_ret * weights).sum(axis=1).mean() * 252\n",
    "print(\"Expected Portfolio Return {} x100(%)\".format(exp_ret))\n",
    "\n",
    "\n",
    "# Expected Volatility\n",
    "exp_vol = np.sqrt((log_ret * weights).sum(axis=1).var() * 252)\n",
    "print(\"Expected Volatility {}\".format(exp_vol))\n",
    "print('\\n')\n",
    "\n",
    "# Sharpe Ratio\n",
    "SR = exp_ret/exp_vol\n",
    "print('Sharpe Ratio {}'.format(SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe(df, d1, d2):\n",
    "    \n",
    "    df2 = df.loc[d1:d2]\n",
    "    \n",
    "    # Expected Return\n",
    "    log_ret = df2[[sym + ' log_ret' for sym in symbols]]\n",
    "    log_ret.columns = range(len(log_ret.columns))\n",
    "    weights = df2[[sym + ' weight' for sym in symbols]]\n",
    "    weights.columns = range(len(weights.columns))\n",
    "    exp_ret = (log_ret * weights).sum(axis=1).mean() * 252\n",
    "\n",
    "    # Expected Volatility\n",
    "    exp_vol = np.sqrt((log_ret * weights).sum(axis=1).var() * 252)\n",
    "\n",
    "    # Sharpe Ratio\n",
    "    SR = exp_ret/exp_vol\n",
    "    return SR\n",
    "\n",
    "sharpe(df, d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date, months = '2014-01-01', '2018-01-01', 12\n",
    "it = util.generate_periods(start_date, end_date, months, months_overlap=0)\n",
    "\n",
    "for d1, d2 in it:\n",
    "    print(sharpe(df, d1, d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def get_ret_vol_sr(weights, df):\n",
    "    \"\"\"\n",
    "    Takes in weights, returns array or return,volatility, sharpe ratio\n",
    "    \"\"\"\n",
    "    weights = np.array(weights)\n",
    "    log_ret = df[[sym + ' log_ret' for sym in symbols]]\n",
    "    ret = (log_ret * weights).sum(axis=1).mean() * 252\n",
    "    vol = np.sqrt((log_ret * weights).sum(axis=1).var() * 252)\n",
    "    sr = ret/vol\n",
    "    return np.array([ret,vol,sr])\n",
    "\n",
    "def neg_sharpe(weights, df):\n",
    "    return  get_ret_vol_sr(weights, df)[2] * -1\n",
    "\n",
    "# Contraints\n",
    "def check_sum(weights):\n",
    "    '''\n",
    "    Returns 0 if sum of weights is 1.0\n",
    "    '''\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "# By convention of minimize function it should be a function that returns zero for conditions\n",
    "cons = ({'type':'eq','fun': check_sum})\n",
    "# 0-1 bounds for each weight\n",
    "bounds = ((0,1),) * len(symbols)\n",
    "# Initial Guess (equal distribution)\n",
    "init_guess = [1/len(symbols)] * len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Least SQuares Programming (SLSQP).\n",
    "opt_results = minimize(neg_sharpe, init_guess, args=(df2), method='SLSQP', bounds=bounds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(symbols)\n",
    "print(np.round(opt_results.x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ret vol sr\")\n",
    "print(np.round(get_ret_vol_sr(opt_results.x, df2), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_optimal_weights(df, date_from, date_to, freq=3, lookback=6):\n",
    "    if (parse(date_from) - df.index[0]).days < 30 * lookback:\n",
    "        raise Exception(\"Cannot lookback {} months from beginning of Time Series {}\".format(lookback, df.index[0]))\n",
    "    if parse(date_from) >= df.index[-1]:\n",
    "        raise Exception(\"date_from {} is after the end of Time Series {}\".format(date_from, df.index[-1]))\n",
    "    d1 = parse(date_from) - datetime.timedelta(30 * lookback)\n",
    "    d2 = parse(date_from)\n",
    "    \n",
    "    i, max = 0, 10\n",
    "    while d2 <= parse(date_to) and i < max:\n",
    "        i += 1\n",
    "        df2 = df.loc[d1:d2]\n",
    "        print(\"Minimizing for dates {} {}\". format(d1, d2))\n",
    "        opt_results = minimize(neg_sharpe, init_guess, args=(df2), method='SLSQP', bounds=bounds, constraints=cons)\n",
    "\n",
    "        print(\"Symbols: {}\".format(symbols))\n",
    "        print(\"Weights: {}\".format(np.round(opt_results.x, 5)))\n",
    "        \n",
    "        d1, d2 = d2, datetime.datetime.combine(util.add_months(d2, freq), datetime.datetime.min.time())\n",
    "        #print(opt_results)\n",
    "\n",
    "generate_optimal_weights(df2, '2015-01-01', '2017-01-01')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
